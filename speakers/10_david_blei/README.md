# Variational Inference: Foundations and Innovations

## Abstract

One of the core problems of modern statistics and machine learning is to approximate difficult-to-compute probability distributions. This problem is especially important in probabilistic modeling and Bayesian statistics, which frame all inference about unknown quantities as calculations about conditional distributions. In this tutorial I will review and discuss variational inference (VI), a method a that approximates probability distributions through optimization. VI has been used in myriad applications in machine learning and tends to be faster than more traditional methods, such as Markov chain Monte Carlo sampling. I will first review the basics of variational inference. Then I will describe some of the pivotal tools for VI that have been developed in the last few years: Monte Carlo gradients, black box variational inference, stochastic variational inference, and variational autoencoders. Last, I will discuss some of the unsolved problems in VI and point to promising research directions.